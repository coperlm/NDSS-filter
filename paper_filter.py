#!/usr/bin/env python3
"""
AI+è§„åˆ™è¾…åŠ©ç­›é€‰å™¨ - NDSS 2025è®ºæ–‡æ™ºèƒ½ç­›é€‰å·¥å…·
åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å’Œè§„åˆ™çš„è®ºæ–‡æ¨èç³»ç»Ÿ
"""

import json
import numpy as np
from typing import List, Dict, Tuple
import re
from dataclasses import dataclass
from sentence_transformers import SentenceTransformer, util
import torch

@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: str
    abstract: str
    url: str
    similarity_score: float = 0.0
    rule_score: float = 0.0
    final_score: float = 0.0

class PaperFilter:
    """è®ºæ–‡ç­›é€‰å™¨ç±»"""
    
    def __init__(self, model_name: str = 'paraphrase-MiniLM-L6-v2'):
        """
        åˆå§‹åŒ–ç­›é€‰å™¨
        
        Args:
            model_name: ä½¿ç”¨çš„å¥å­åµŒå…¥æ¨¡å‹åç§°
        """
        print(f"æ­£åœ¨åŠ è½½è¯­ä¹‰åµŒå…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        print("æ¨¡å‹åŠ è½½å®Œæˆ!")
        
        # å®šä¹‰å…³é”®è¯æƒé‡æ˜ å°„
        self.keyword_weights = {
            # å¯†ç å­¦ç›¸å…³ - æ‚¨æ„Ÿå…´è¶£çš„é¢†åŸŸ
            'zero knowledge': 1.5,  # é›¶çŸ¥è¯†è¯æ˜ - é«˜æƒé‡
            'chameleon hash': 1.5,  # å˜è‰²é¾™å“ˆå¸Œ - é«˜æƒé‡
            'public key': 1.3,      # å…¬é’¥å¯†ç å­¦
            'cryptography': 1.2, 
            'encryption': 1.2,
            'digital signature': 1.3,  # æ•°å­—ç­¾å
            'elliptic curve': 1.2,     # æ¤­åœ†æ›²çº¿å¯†ç å­¦
            'rsa': 1.1,                # RSAç®—æ³•
            'bilinear pairing': 1.2,   # åŒçº¿æ€§é…å¯¹
            'lattice': 1.1,            # æ ¼å¯†ç å­¦
            'homomorphic': 1.0,        # åŒæ€åŠ å¯†
            'commitment': 1.2,         # æ‰¿è¯ºæ–¹æ¡ˆ
            'proof system': 1.3,       # è¯æ˜ç³»ç»Ÿ
            'cryptographic protocol': 1.2,  # å¯†ç åè®®
            'hash function': 1.1,      # å“ˆå¸Œå‡½æ•°
            'merkle tree': 1.0,        # é»˜å…‹å°”æ ‘
            'privacy': 0.8,            # éšç§ç›¸å…³ä½†æƒé‡ç¨ä½
            'secure': 0.7,             # å®‰å…¨ç›¸å…³ä½†æƒé‡è¾ƒä½
            
            # æœºå™¨å­¦ä¹ å®‰å…¨ - ä¸æ„Ÿå…´è¶£
            'adversarial': 0.0,
            'neural network': 0.0,
            'deep learning': 0.0,
            'machine learning': 0.0,
            'federated learning': 0.0,
            'artificial intelligence': 0.0,
            
            # ç³»ç»Ÿå®‰å…¨ - ä¸æ„Ÿå…´è¶£
            'vulnerability': 0.0,
            'attack': 0.0,
            'defense': 0.0,
            'malware': 0.0,
            'penetration': 0.0,
            'exploit': 0.0,
            
            # åŒºå—é“¾ç›¸å…³ - ä¸­ç­‰å…´è¶£ï¼ˆå› ä¸ºæ¶‰åŠå¯†ç å­¦ï¼‰
            'blockchain': 0.6,
            'bitcoin': 0.5,
            'cryptocurrency': 0.5,
            'smart contract': 0.4,
        }
    
    def load_papers_from_json(self, json_file: str) -> List[Paper]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½è®ºæ–‡æ•°æ®
        
        Args:
            json_file: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            è®ºæ–‡å¯¹è±¡åˆ—è¡¨
        """
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            papers = []
            for item in data:
                paper = Paper(
                    title=item.get('title', ''),
                    authors=item.get('authors', ''),
                    abstract=item.get('abstract', ''),
                    url=item.get('url', '')
                )
                papers.append(paper)
            
            print(f"æˆåŠŸåŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"åŠ è½½JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []
    
    def calculate_semantic_similarity(self, research_interest: str, papers: List[Paper]) -> List[Paper]:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        
        Args:
            research_interest: ç ”ç©¶å…´è¶£æè¿°
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            æ›´æ–°äº†ç›¸ä¼¼åº¦åˆ†æ•°çš„è®ºæ–‡åˆ—è¡¨
        """
        print("æ­£åœ¨è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦...")
        
        # ç¼–ç ç ”ç©¶å…´è¶£
        interest_embedding = self.model.encode(research_interest, convert_to_tensor=True)
        
        # ç¼–ç æ‰€æœ‰è®ºæ–‡æ‘˜è¦
        abstracts = [paper.abstract for paper in papers]
        abstract_embeddings = self.model.encode(abstracts, convert_to_tensor=True)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cos_scores = util.cos_sim(interest_embedding, abstract_embeddings)[0]
        
        # æ›´æ–°è®ºæ–‡çš„ç›¸ä¼¼åº¦åˆ†æ•°
        for i, paper in enumerate(papers):
            paper.similarity_score = float(cos_scores[i])
        
        print("è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ!")
        return papers
    
    def apply_rule_based_filtering(self, papers: List[Paper]) -> List[Paper]:
        """
        åº”ç”¨åŸºäºè§„åˆ™çš„ç­›é€‰
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            æ›´æ–°äº†è§„åˆ™åˆ†æ•°çš„è®ºæ–‡åˆ—è¡¨
        """
        print("æ­£åœ¨åº”ç”¨è§„åˆ™ç­›é€‰...")
        
        for paper in papers:
            rule_score = 0.0
            
            # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œå…³é”®è¯åŒ¹é…
            text = (paper.title + " " + paper.abstract).lower()
            
            # æ ¹æ®å…³é”®è¯è®¡ç®—è§„åˆ™åˆ†æ•°
            for keyword, weight in self.keyword_weights.items():
                if keyword in text:
                    rule_score += weight
                    
            # æ ‡é¢˜ä¸­çš„å…³é”®è¯ç»™äºˆé¢å¤–æƒé‡
            title_lower = paper.title.lower()
            for keyword, weight in self.keyword_weights.items():
                if keyword in title_lower and weight > 0:
                    rule_score += weight * 0.5  # æ ‡é¢˜å…³é”®è¯é¢å¤–åŠ åˆ†
            
            paper.rule_score = rule_score
        
        print("è§„åˆ™ç­›é€‰å®Œæˆ!")
        return papers
    
    def calculate_final_scores(self, papers: List[Paper], 
                             semantic_weight: float = 0.7, 
                             rule_weight: float = 0.3) -> List[Paper]:
        """
        è®¡ç®—æœ€ç»ˆç»¼åˆåˆ†æ•°
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            semantic_weight: è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
            rule_weight: è§„åˆ™åˆ†æ•°æƒé‡
            
        Returns:
            æ›´æ–°äº†æœ€ç»ˆåˆ†æ•°çš„è®ºæ–‡åˆ—è¡¨
        """
        print("æ­£åœ¨è®¡ç®—æœ€ç»ˆç»¼åˆåˆ†æ•°...")
        
        # å½’ä¸€åŒ–è¯­ä¹‰åˆ†æ•°åˆ°0-1èŒƒå›´
        semantic_scores = [paper.similarity_score for paper in papers]
        if semantic_scores:
            min_sem = min(semantic_scores)
            max_sem = max(semantic_scores)
            sem_range = max_sem - min_sem if max_sem != min_sem else 1
        
        # å½’ä¸€åŒ–è§„åˆ™åˆ†æ•°åˆ°0-1èŒƒå›´
        rule_scores = [paper.rule_score for paper in papers]
        if rule_scores:
            min_rule = min(rule_scores)
            max_rule = max(rule_scores)
            rule_range = max_rule - min_rule if max_rule != min_rule else 1
        
        for paper in papers:
            # å½’ä¸€åŒ–åˆ†æ•°
            norm_semantic = (paper.similarity_score - min_sem) / sem_range if sem_range > 0 else 0
            norm_rule = (paper.rule_score - min_rule) / rule_range if rule_range > 0 else 0
            
            # è®¡ç®—åŠ æƒç»¼åˆåˆ†æ•°
            paper.final_score = semantic_weight * norm_semantic + rule_weight * norm_rule
        
        print("æœ€ç»ˆåˆ†æ•°è®¡ç®—å®Œæˆ!")
        return papers
    
    def filter_and_rank(self, research_interest: str, papers: List[Paper], 
                       top_k: int = 10, semantic_weight: float = 0.7) -> List[Paper]:
        """
        å®Œæ•´çš„ç­›é€‰å’Œæ’åºæµç¨‹
        
        Args:
            research_interest: ç ”ç©¶å…´è¶£æè¿°
            papers: è®ºæ–‡åˆ—è¡¨
            top_k: è¿”å›å‰kç¯‡è®ºæ–‡
            semantic_weight: è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
            
        Returns:
            æ’åºåçš„å‰kç¯‡è®ºæ–‡
        """
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        papers = self.calculate_semantic_similarity(research_interest, papers)
        
        # åº”ç”¨è§„åˆ™ç­›é€‰
        papers = self.apply_rule_based_filtering(papers)
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        papers = self.calculate_final_scores(papers, semantic_weight, 1 - semantic_weight)
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        papers.sort(key=lambda x: x.final_score, reverse=True)
        
        return papers[:top_k]
    
    def print_results(self, papers: List[Paper], show_scores: bool = True):
        """
        æ‰“å°ç­›é€‰ç»“æœ
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            show_scores: æ˜¯å¦æ˜¾ç¤ºåˆ†æ•°è¯¦æƒ…
        """
        print("\n" + "="*80)
        print("ğŸ¯ è®ºæ–‡æ¨èç»“æœ")
        print("="*80)
        
        for i, paper in enumerate(papers, 1):
            print(f"\nğŸ“„ ã€ç¬¬ {i} åã€‘")
            print(f"æ ‡é¢˜: {paper.title}")
            print(f"ä½œè€…: {paper.authors}")
            
            if show_scores:
                print(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {paper.similarity_score:.4f}")
                print(f"è§„åˆ™åˆ†æ•°: {paper.rule_score:.2f}")
                print(f"ç»¼åˆåˆ†æ•°: {paper.final_score:.4f}")
            
            # æ˜¾ç¤ºæ‘˜è¦å‰200ä¸ªå­—ç¬¦
            abstract_preview = paper.abstract[:200] + "..." if len(paper.abstract) > 200 else paper.abstract
            print(f"æ‘˜è¦é¢„è§ˆ: {abstract_preview}")
            print(f"é“¾æ¥: {paper.url}")
            print("-" * 60)
    
    def export_results(self, papers: List[Paper], output_file: str):
        """
        å¯¼å‡ºç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        results = []
        for paper in papers:
            results.append({
                'title': paper.title,
                'authors': paper.authors,
                'abstract': paper.abstract,
                'url': paper.url,
                'similarity_score': paper.similarity_score,
                'rule_score': paper.rule_score,
                'final_score': paper.final_score
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4, ensure_ascii=False)
        
        print(f"\nç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä½¿ç”¨æ–¹æ³•"""
    print("ğŸš€ NDSS 2025 è®ºæ–‡æ™ºèƒ½ç­›é€‰å™¨")
    print("="*50)
    
    # åˆå§‹åŒ–ç­›é€‰å™¨
    filter_system = PaperFilter()
    
    # åŠ è½½è®ºæ–‡æ•°æ®
    papers = filter_system.load_papers_from_json('ndss_papers_2025.json')
    
    if not papers:
        print("âŒ æœªèƒ½åŠ è½½è®ºæ–‡æ•°æ®ï¼Œè¯·æ£€æŸ¥JSONæ–‡ä»¶")
        return
    
    # è®¾ç½®é»˜è®¤ç ”ç©¶å…´è¶£ - æ‚¨å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹ä¸ºæ‚¨æ„Ÿå…´è¶£çš„é¢†åŸŸ
    default_research_interest = "zero-knowledge proofs, chameleon hash functions, public key cryptography, digital signatures, and cryptographic protocols"
    
    # å¯é€‰çš„ç ”ç©¶å…´è¶£åˆ—è¡¨ï¼ˆä»…ä¾›å‚è€ƒï¼Œç¨‹åºä¼šä½¿ç”¨é»˜è®¤å€¼ï¼‰
    research_interests = [
        "zero-knowledge proofs and cryptographic protocols",
        "chameleon hash functions and commitment schemes",
        "public key cryptography and digital signatures", 
        "elliptic curve cryptography and bilinear pairings",
        "lattice-based cryptography and post-quantum security",
    ]
    
    # ä½¿ç”¨é»˜è®¤ç ”ç©¶å…´è¶£
    research_interest = default_research_interest
    print(f"ğŸ¯ ä½¿ç”¨é»˜è®¤ç ”ç©¶å…´è¶£: {research_interest}")
    print("ğŸ’¡ å¦‚éœ€ä¿®æ”¹ç ”ç©¶å…´è¶£ï¼Œè¯·ç¼–è¾‘ä»£ç ä¸­çš„ default_research_interest å˜é‡")
    print("ğŸ” å½“å‰åå¥½: é›¶çŸ¥è¯†è¯æ˜ã€å˜è‰²é¾™å“ˆå¸Œã€å…¬é’¥å¯†ç å­¦ç›¸å…³è®ºæ–‡")
    
    try:
        
        # æ‰§è¡Œç­›é€‰
        top_papers = filter_system.filter_and_rank(
            research_interest=research_interest,
            papers=papers,
            top_k=10,  # è¿”å›å‰10ç¯‡
            semantic_weight=0.7  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡70%ï¼Œè§„åˆ™æƒé‡30%
        )
        
        # æ˜¾ç¤ºç»“æœ
        filter_system.print_results(top_papers)
        
        # å¯¼å‡ºç»“æœ
        output_file = f"filtered_papers_{len(top_papers)}.json"
        filter_system.export_results(top_papers, output_file)
        
        print(f"\nâœ… ç­›é€‰å®Œæˆ! ä» {len(papers)} ç¯‡è®ºæ–‡ä¸­ä¸ºæ‚¨æ¨èäº† {len(top_papers)} ç¯‡æœ€ç›¸å…³çš„è®ºæ–‡ã€‚")
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main()
